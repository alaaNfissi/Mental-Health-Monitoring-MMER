{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938e3a7a-e6ad-4f3e-8e02-7ff781a3a472",
   "metadata": {},
   "source": [
    "Extract samples names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81672ff-ec3e-42b4-a18f-d37f038f011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_nested_subfolder_names(directory):\n",
    "    nested_subfolder_list = []\n",
    "    \n",
    "    for parent_folder in os.listdir(directory):\n",
    "        parent_path = os.path.join(directory, parent_folder)\n",
    "        if os.path.isdir(parent_path):\n",
    "            for subfolder in os.listdir(parent_path):\n",
    "                subfolder_path = os.path.join(parent_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    nested_subfolder_list.append(subfolder)\n",
    "    \n",
    "    return nested_subfolder_list\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\IEMOCAP_RAW_PROCESSED\"  # Change this to your target directory\n",
    "nested_subfolders = get_nested_subfolder_names(directory_path)\n",
    "\n",
    "# Print all second-level folder names\n",
    "for nested_subfolder in nested_subfolders:\n",
    "    print(nested_subfolder)\n",
    "\n",
    "# Save second-level folder names to a file\n",
    "with open(\"D:\\\\Nouveau dossier\\\\data\\\\data\\\\folder_names.txt\", \"w\") as f:\n",
    "    for nested_subfolder in nested_subfolders:\n",
    "        f.write(nested_subfolder + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f4c4b-08a8-409e-b373-8216939b98e8",
   "metadata": {},
   "source": [
    "Open the data (meta.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2bf012-5754-4bf1-8c53-c6ee3a4a2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Step 1: Load the data from meta.pkl\n",
    "\n",
    "file_path='D:\\\\Nouveau dossier\\\\data\\\\data\\\\IEMOCAP_RAW_PROCESSED\\\\meta.pkl' #replace the path\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3f98e-3e71-40e7-a2a2-d3fbbb21f286",
   "metadata": {},
   "source": [
    "Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28e1f7f-9fbf-4e78-b483-14e522c67790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neu': 1708,\n",
       "         'xxx': 2507,\n",
       "         'fru': 1849,\n",
       "         'ang': 1103,\n",
       "         'sad': 1084,\n",
       "         'hap': 595,\n",
       "         'exc': 1041,\n",
       "         'sur': 107,\n",
       "         'oth': 3,\n",
       "         'fea': 40,\n",
       "         'dis': 2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# File path containing the list of keys\n",
    "file_path = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\folder_names.txt\"\n",
    "\n",
    "# Read the list of keys from the file\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        keys_list = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    keys_list = list(data.keys())  # Defaulting to all keys if file is missing\n",
    "\n",
    "# Filter data based on keys in the file\n",
    "filtered_data = {key: data[key] for key in keys_list if key in data}\n",
    "\n",
    "# Count occurrences of each emotion label\n",
    "emotion_counts = Counter(entry['label'] for entry in filtered_data.values())\n",
    "\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a60b1-3701-4345-b2c5-39d66d6dcba7",
   "metadata": {},
   "source": [
    "get those who has : {'neu', 'ang', 'sad', 'exc', 'hap'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf256c-ac6d-44c1-8e66-540c47dcd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target emotions\n",
    "target_emotions = {'neu', 'ang', 'sad', 'exc', 'hap'}\n",
    "\n",
    "# Extract names of entries with one of the target emotions\n",
    "filtered_names = [key for key, entry in filtered_data.items() if entry['label'] in target_emotions]\n",
    "\n",
    "# Print the extracted names\n",
    "print(filtered_names)\n",
    "\n",
    "# Save second-level folder names to a file\n",
    "with open(\"D:\\\\Nouveau dossier\\\\data\\\\data\\\\4emotions_folder_names.txt\", \"w\") as f:\n",
    "    for name in filtered_names:\n",
    "        f.write(name + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1c36f0-4f41-4cc3-99ac-fe907217463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neu': 1708, 'ang': 1103, 'sad': 1084, 'hap': 1636})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# File path containing the list of keys\n",
    "file_path = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\4emotions_folder_names.txt\"\n",
    "\n",
    "# Read the list of keys from the file\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        keys_list = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    keys_list = list(data.keys())  # Defaulting to all keys if file is missing\n",
    "\n",
    "# Filter data based on keys in the file\n",
    "filtered_data = {key: data[key] for key in keys_list if key in data}\n",
    "\n",
    "# Count occurrences of each emotion label\n",
    "emotion_counts = Counter(entry['label'] for entry in filtered_data.values())\n",
    "\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa6b47-eba4-4194-9ca7-6db8f261a585",
   "metadata": {},
   "source": [
    "Change exc to hap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b0fad1f-e6d2-44f8-917b-67cdae1e78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Step 1: Load the data from meta.pkl\n",
    "\n",
    "file_path2='D:\\\\Nouveau dossier\\\\data\\\\data\\\\IEMOCAP_RAW_PROCESSED\\\\meta - exc+hap.pkl' #replace the path where to save the changes\n",
    "file_path='D:\\\\Nouveau dossier\\\\data\\\\data\\\\IEMOCAP_RAW_PROCESSED\\\\meta.pkl' #replace the path where the original file\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc51458d-9a72-4aed-807d-863ec087b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been updated and saved back to D:\\Nouveau dossier\\data\\data\\IEMOCAP_RAW_PROCESSED\\meta - exc+hap.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace label 'exc' with 'hap'\n",
    "for key, value in data.items():\n",
    "    if value['label'] == 'exc':\n",
    "        value['label'] = 'hap'\n",
    "\n",
    "\n",
    "\n",
    "# Keep only the items with labels in ['hap', 'ang', 'sad', 'neu'] from keys_list\n",
    "valid_labels = {'hap', 'ang', 'sad', 'neu'}\n",
    "\n",
    "with open(file_path2, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"Data has been updated and saved back to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb073a7-68b3-4307-ba6e-29e0c4e780b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neu': 1708, 'ang': 1103, 'sad': 1084, 'hap': 1636})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "#verification of the distribution and combination of hap and exc\n",
    "\n",
    "# File path containing the list of keys\n",
    "file_path = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\4emotions_folder_names.txt\"\n",
    "\n",
    "# Read the list of keys from the file\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        keys_list = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    keys_list = list(data.keys())  # Defaulting to all keys if file is missing\n",
    "\n",
    "# Filter data based on keys in the file\n",
    "filtered_data = {key: data[key] for key in keys_list if key in data}\n",
    "\n",
    "# Count occurrences of each emotion label\n",
    "emotion_counts = Counter(entry['label'] for entry in filtered_data.values())\n",
    "\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62376a72-fec6-4f53-b0fc-2f3dbd4ca5a3",
   "metadata": {},
   "source": [
    "extract the train,valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "268f6f7b-609c-44b7-abd9-bc27c94c0f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1636 samples saved to D:\\Nouveau dossier\\data\\data\\hap.txt\n",
      "1103 samples saved to D:\\Nouveau dossier\\data\\data\\ang.txt\n",
      "1708 samples saved to D:\\Nouveau dossier\\data\\data\\neu.txt\n",
      "1084 samples saved to D:\\Nouveau dossier\\data\\data\\sad.txt\n"
     ]
    }
   ],
   "source": [
    "#extract the emotions folder each in its own file\n",
    "\n",
    "# Define file paths\n",
    "input_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\4emotions_folder_names.txt\"\n",
    "\n",
    "# Output files for each emotion\n",
    "output_files = {\n",
    "    \"hap\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\hap.txt\",\n",
    "    \"ang\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\ang.txt\",\n",
    "    \"neu\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\neu.txt\",\n",
    "    \"sad\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\sad.txt\"\n",
    "}\n",
    "\n",
    "# Read the names from the input file\n",
    "if os.path.exists(input_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        file_names = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    file_names = []  # Default to empty list if file is missing\n",
    "\n",
    "# Initialize emotion-wise storage\n",
    "emotion_wise_names = {emotion: [] for emotion in output_files.keys()}\n",
    "\n",
    "# Filter names based on their emotion labels\n",
    "for name in file_names:\n",
    "    if name in data:\n",
    "        emotion_label = data[name]['label']\n",
    "        if emotion_label in emotion_wise_names:\n",
    "            emotion_wise_names[emotion_label].append(name)\n",
    "\n",
    "# Write the filtered names to respective files\n",
    "for emotion, file_path in output_files.items():\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(emotion_wise_names[emotion]))\n",
    "\n",
    "# Print confirmation messages\n",
    "for emotion, file_path in output_files.items():\n",
    "    print(f\"{len(emotion_wise_names[emotion])} samples saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135dc09a-39b1-4872-94a8-f328ba3eb7d7",
   "metadata": {},
   "source": [
    "extract 90% from the file (one emotion:hap) into train, 5percent into valid, and 5percent intotest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ee1ac86-3f60-4956-a4b2-11dcf6fc1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split: 1472 samples saved to D:\\Nouveau dossier\\data\\data\\train_split.txt\n",
      "Validation split: 81 samples saved to D:\\Nouveau dossier\\data\\data\\valid_split.txt\n",
      "Test split: 83 samples saved to D:\\Nouveau dossier\\data\\data\\test_split.txt\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "hap_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\hap.txt\"\n",
    "train_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\train_split.txt\"\n",
    "valid_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\valid_split.txt\"\n",
    "test_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\test_split.txt\"\n",
    "\n",
    "# Read the names from hap.txt\n",
    "if os.path.exists(hap_file):\n",
    "    with open(hap_file, \"r\") as f:\n",
    "        hap_names = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    hap_names = []  # Default to empty list if file is missing\n",
    "\n",
    "\n",
    "\n",
    "# Compute split sizes\n",
    "total_count = len(hap_names)\n",
    "train_count = int(0.9 * total_count)\n",
    "valid_count = int(0.05 * total_count)\n",
    "test_count = total_count - (train_count + valid_count)  # Ensure full allocation\n",
    "\n",
    "# Assign samples to each split\n",
    "train_split = hap_names[:train_count]\n",
    "valid_split = hap_names[train_count:train_count + valid_count]\n",
    "test_split = hap_names[train_count + valid_count:]\n",
    "\n",
    "# Save the splits to respective files\n",
    "for file_path, split_data in zip([train_file, valid_file, test_file], \n",
    "                                 [train_split, valid_split, test_split]):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(split_data))\n",
    "\n",
    "# Print confirmation with the count per split\n",
    "print(f\"Train split: {len(train_split)} samples saved to {train_file}\")\n",
    "print(f\"Validation split: {len(valid_split)} samples saved to {valid_file}\")\n",
    "print(f\"Test split: {len(test_split)} samples saved to {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e1b10-997c-46ca-b44d-b9be983c1cc9",
   "metadata": {},
   "source": [
    "add other emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "672a99f4-e92c-4eae-9f6d-648776c532f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Train split: 4976 samples saved to D:\\Nouveau dossier\\data\\data\\train_split.txt\n",
      "Updated Validation split: 275 samples saved to D:\\Nouveau dossier\\data\\data\\valid_split.txt\n",
      "Updated Test split: 280 samples saved to D:\\Nouveau dossier\\data\\data\\test_split.txt\n"
     ]
    }
   ],
   "source": [
    "# Define file path for neu.txt\n",
    "neu_file = \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\sad.txt\" # change this to other paths\n",
    "\n",
    "# Read the names from neu.txt\n",
    "if os.path.exists(neu_file):\n",
    "    with open(neu_file, \"r\") as f:\n",
    "        neu_names = [line.strip() for line in f.readlines()]\n",
    "else:\n",
    "    neu_names = []  # Default to empty list if file is missing\n",
    "\n",
    "\n",
    "\n",
    "# Compute split sizes for neu\n",
    "total_neu_count = len(neu_names)\n",
    "train_neu_count = int(0.9 * total_neu_count)\n",
    "valid_neu_count = int(0.05 * total_neu_count)\n",
    "test_neu_count = total_neu_count - (train_neu_count + valid_neu_count)  # Ensure full allocation\n",
    "\n",
    "# Assign neu samples to each split\n",
    "train_split.extend(neu_names[:train_neu_count])\n",
    "valid_split.extend(neu_names[train_neu_count:train_neu_count + valid_neu_count])\n",
    "test_split.extend(neu_names[train_neu_count + valid_neu_count:])\n",
    "\n",
    "# Save the updated splits to respective files\n",
    "for file_path, split_data in zip([train_file, valid_file, test_file], \n",
    "                                 [train_split, valid_split, test_split]):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(split_data))\n",
    "\n",
    "# Print confirmation with the updated count per split\n",
    "print(f\"Updated Train split: {len(train_split)} samples saved to {train_file}\")\n",
    "print(f\"Updated Validation split: {len(valid_split)} samples saved to {valid_file}\")\n",
    "print(f\"Updated Test split: {len(test_split)} samples saved to {test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b41c8c-d788-4ac2-aaa8-b677622c91fb",
   "metadata": {},
   "source": [
    "verification of distribution in the train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35336995-38c5-4d52-affd-dcd11273d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Emotion Distribution:\n",
      " {'hap': 1472, 'neu': 1537, 'ang': 992, 'sad': 975} \n",
      "\n",
      "Test Split Emotion Distribution:\n",
      " {'hap': 83, 'neu': 86, 'ang': 56, 'sad': 55} \n",
      "\n",
      "Validation Split Emotion Distribution:\n",
      " {'hap': 81, 'neu': 85, 'ang': 55, 'sad': 54} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for train, test, and validation splits\n",
    "split_files = {\n",
    "    \"Train Split\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\train_split.txt\",\n",
    "    \"Test Split\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\test_split.txt\",\n",
    "    \"Validation Split\": \"D:\\\\Nouveau dossier\\\\data\\\\data\\\\valid_split.txt\",\n",
    "}\n",
    "\n",
    "# Dictionary to store emotion distributions\n",
    "split_emotion_counts = {}\n",
    "\n",
    "# Process each split file\n",
    "for split_name, file_path in split_files.items():\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            keys_list = [line.strip() for line in f.readlines()]\n",
    "\n",
    "        # Filter data based on keys in the file\n",
    "        filtered_data = {key: data[key] for key in keys_list if key in data}\n",
    "\n",
    "        # Count occurrences of each emotion label\n",
    "        emotion_counts = Counter(entry['label'] for entry in filtered_data.values())\n",
    "\n",
    "        # Store results\n",
    "        split_emotion_counts[split_name] = dict(emotion_counts)\n",
    "    else:\n",
    "        split_emotion_counts[split_name] = \"File not found\"\n",
    "\n",
    "# Print the emotion distributions for each split\n",
    "for split_name, counts in split_emotion_counts.items():\n",
    "    print(f\"{split_name} Emotion Distribution:\\n\", counts, \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
